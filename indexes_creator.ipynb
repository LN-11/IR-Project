{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5e9a5e",
   "metadata": {
    "id": "4a5e9a5e"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b9c70",
   "metadata": {
    "id": "779b9c70",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
   },
   "outputs": [],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023d0b92",
   "metadata": {
    "id": "023d0b92",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dfb4190",
   "metadata": {
    "id": "1dfb4190",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import hashlib\n",
    "#from google.colab import auth\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54537d2",
   "metadata": {
    "id": "f54537d2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-pyspark-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fbc3e7e",
   "metadata": {
    "id": "2fbc3e7e",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-spark-version",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-4a1a-m.c.effective-balm-374918.internal:33155\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7e651a7520>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c961eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = 'ln3250'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name[0] == 'm':\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a17a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f2681e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wiki_corpus = spark.read.parquet(*paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63056a",
   "metadata": {
    "id": "2e63056a"
   },
   "source": [
    "# Inverted indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f4e857",
   "metadata": {
    "id": "d7f4e857"
   },
   "outputs": [],
   "source": [
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "def _hash(s):\n",
    "    \"\"\"\n",
    "    :param s: term\n",
    "    :return: hashed bucket_id\n",
    "    \"\"\"\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    tokenizes the text and removes stopwords\n",
    "    :param text: text\n",
    "    :return: text in a tokenized format, stopwords removed\n",
    "    \"\"\"\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens_filtered = [term for term in tokens if term not in all_stopwords]\n",
    "    return tokens_filtered\n",
    "\n",
    "\n",
    "def word_count(text, doc_id):\n",
    "    \"\"\"\n",
    "    counts the doc freq for each term\n",
    "    :param text: text\n",
    "    :param doc_id: doc_id\n",
    "    :return: list in the form of [(term ,(doc_id, cnt))...] for each term in the doc\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    tok_val_cnt = dict(Counter(tokens))\n",
    "    return [(term ,(doc_id, cnt)) for term, cnt in tok_val_cnt.items()]\n",
    "\n",
    "\n",
    "def doc_len(text, doc_id):\n",
    "    \"\"\"\n",
    "    counts the number of words in a document\n",
    "    :param text: text\n",
    "    :param doc_id: doc_id\n",
    "    :return: tuple of (doc_id, doc_len)\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    return doc_id, len(tokens)\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"\n",
    "    sorts the pl by the frequency\n",
    "    :param unsorted_pl: unsorted_pl\n",
    "    :return: sorted pl\n",
    "    \"\"\"\n",
    "    return sorted(unsorted_pl, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "def token2bucket_id(token, bucket_num):\n",
    "    \"\"\"\n",
    "    convert token to bucket id via hash\n",
    "    :param token: token\n",
    "    :param bucket_num: num of buckets to create\n",
    "    :return: hashed token (number)\n",
    "    \"\"\"\n",
    "    return int(_hash(token),16) % bucket_num\n",
    "\n",
    "def partition_postings_and_write(postings, bucket_name, bucket_num, ii = InvertedIndex()):\n",
    "    \"\"\"\n",
    "    writing the posting list to the bucket\n",
    "    :param postings:\n",
    "    :param bucket_name:\n",
    "    :param bucket_num:\n",
    "    :param ii:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bucket_docs = postings.map(lambda x: (token2bucket_id(x[0], bucket_num), x)).groupByKey()\n",
    "    return bucket_docs.map(lambda x: ii.write_a_posting_list(x, bucket_name)) # GCP line\n",
    "\n",
    "def calculate_df(posting_lists):\n",
    "    \"\"\"\n",
    "    calculate the number of documents that contain each term\n",
    "    :param posting_lists: posting_lists\n",
    "    :return: tuple of (term, df)\n",
    "    \"\"\"\n",
    "    return posting_lists.mapValues(len)\n",
    "\n",
    "\n",
    "def doc_norm(text, doc_id ,df , N):\n",
    "    \"\"\"\n",
    "    calculate the doc norm in order to calculate the cosine similarity in the future\n",
    "    :param text: text\n",
    "    :param doc_id: doc id\n",
    "    :param df: list in the form of (term, doc freq')...\n",
    "    :param N: corpus size\n",
    "    :return: the doc norm\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    dl = len(tokens)\n",
    "    term_cnt = Counter(tokens)\n",
    "    norm = 0\n",
    "    for term, cnt in term_cnt.items():\n",
    "        if term in df:\n",
    "            norm += (cnt / dl * np.log(N/df[term]))**2\n",
    "    return doc_id, np.sqrt(norm)\n",
    "\n",
    "\n",
    "def index_writing(ii, bucket_name):\n",
    "# write the inverted index instance as a pkl to the bucket\n",
    "  ii.write_index('.', f'index_{bucket_name}')\n",
    "  index_src = f\"index_{bucket_name}.pkl\"\n",
    "  index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "  !gsutil cp $index_src $index_dst\n",
    "\n",
    "def postings_writing(bucket_name):\n",
    "    # getting the final paths to the postings list which will be linked to the inverted index instance.\n",
    "  super_posting_locs = defaultdict(list)\n",
    "  for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "      continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "      posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "       super_posting_locs[k].extend(v)\n",
    "  return super_posting_locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "545f9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(wiki_corpus, bucket_name, index_on, bucket_num,  filter_size):\n",
    "    \"\"\"\n",
    "    create the inverted index and the postings\n",
    "    :param wiki_corpus: wiki corpus\n",
    "    :param bucket_name: bucket to write stuff into\n",
    "    :param index_on: one of \"title\", \"text\", \"anchor_text\"\n",
    "    :param bucket_num: number of buckets\n",
    "    :param filter_size: determines the minimum frequency for terms\n",
    "    \"\"\"\n",
    "    if index_on == \"anchor\":\n",
    "        doc_item_pairs = wiki_corpus.select(index_on+\"_text\").rdd\\\n",
    "        .flatMap(lambda x: x['anchor_text']).reduceByKey(lambda x, y: x + \" \" + y)\\\n",
    "        .map(lambda x: (x[1], x[0]))\n",
    "    else:\n",
    "        doc_item_pairs = wiki_corpus.select(index_on, \"id\").rdd\n",
    "    \n",
    "    corpus_size = wiki_corpus.count()\n",
    "    word_counts = doc_item_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    postings_filtered = postings.filter(lambda x: len(x[1]) > filter_size)\n",
    "    posting_locs_list = partition_postings_and_write(postings_filtered, bucket_name, bucket_num).collect()\n",
    "    super_posting_locs = postings_writing(bucket_name)\n",
    "\n",
    "    d2dl = doc_item_pairs.map(lambda x: doc_len(x[0], x[1]))\n",
    "    w2df = calculate_df(postings_filtered)\n",
    "\n",
    "    d2dl_dict = dict(d2dl.collect())\n",
    "    w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "    d2tfidf_norm = doc_item_pairs.map(lambda x: doc_norm(x[0], x[1], w2df_dict, corpus_size))\n",
    "    d2tfidf_norm_dict = d2tfidf_norm.collectAsMap()\n",
    "\n",
    "    inverted = InvertedIndex()\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "\n",
    "    inverted.df = w2df_dict\n",
    "    inverted.dl = d2dl_dict\n",
    "    inverted.d_norms = d2tfidf_norm_dict\n",
    "\n",
    "    index_writing(inverted, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba412a",
   "metadata": {
    "id": "1bba412a",
    "outputId": "f46e6cf9-5357-4566-ffbe-ea430d43f2fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====>                                                  (10 + 16) / 124]\r"
     ]
    }
   ],
   "source": [
    "#Create inverted indexes\n",
    "create_index(wiki_corpus, 'title_idx', \"title\", 124, filter_size = 0)\n",
    "print(\"title index created\")\n",
    "create_index(wiki_corpus, 'anchor_idx', \"anchor\", 248, filter_size = 20)\n",
    "print(\"anchor index created\")\n",
    "create_index(wiki_corpus, 'body_idx_dan_2', \"text\", 248, filter_size = 50)\n",
    "print(\"body index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7cb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b303ea",
   "metadata": {
    "id": "86b303ea"
   },
   "source": [
    "# Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff53cae",
   "metadata": {
    "id": "4ff53cae"
   },
   "outputs": [],
   "source": [
    "def generate_graph(pages):\n",
    "  edges = pages.flatMap(lambda x: map(lambda y: (x[0], y[0]), x[1])).distinct()\n",
    "  vertices = edges.flatMap(lambda x: x).distinct().map(lambda x:(x,))\n",
    "  return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8f944",
   "metadata": {
    "id": "33b8f944"
   },
   "outputs": [],
   "source": [
    "pages_links = corpus.select(\"id\", \"anchor_text\").rdd\n",
    "# construct the graph \n",
    "edges, vertices = generate_graph(pages_links)\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fe2fb",
   "metadata": {
    "id": "cf7fe2fb"
   },
   "outputs": [],
   "source": [
    "# Store the page rank in bucket\n",
    "pr.repartition(1).write.csv('gs://title_inverted_index/pr', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9663128",
   "metadata": {},
   "source": [
    "# Page views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "# Using user page views (as opposed to spiders and automated traffic) for the \n",
    "# month of August 2021\n",
    "bucket_name = 'ln3250'\n",
    "# Paths\n",
    "# Using user page views (as opposed to spiders and automated traffic) for the \n",
    "# month of August 2021\n",
    "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "p = Path(pv_path) \n",
    "pv_name = p.name\n",
    "pv_temp = f'{p.stem}-4dedup.txt'\n",
    "pv_clean = f'{p.stem}.pkl'\n",
    "# Download the file (2.3GB) \n",
    "!wget -N $pv_path\n",
    "# Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
    "# total number of page views (5). Then, remove lines with article id or page \n",
    "# view values that are not a sequence of digits.\n",
    "!bzcat $pv_name | grep \"^en.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
    "# Create a Counter (dictionary) that sums up the pages views for the same \n",
    "# article, resulting in a mapping from article id to total page views.\n",
    "wid2pv = Counter()\n",
    "with open(pv_temp, 'rt') as f:\n",
    "  for line in f:\n",
    "    parts = line.split(' ')\n",
    "    wid2pv.update({int(parts[0]): int(parts[1])})\n",
    "# write out the counter as binary file (pickle it)\n",
    "with open(pv_clean, 'wb') as f:\n",
    "  pickle.dump(wid2pv, f)\n",
    "\n",
    "\n",
    "# read in the counter\n",
    "# with open(pv_clean, 'rb') as f:\n",
    "#   wid2pv = pickle.loads(f.read())\n",
    "# Create a Storage client\n",
    "auth.authenticate_user()\n",
    "storage_client = storage.Client()\n",
    "# Get a reference to the bucket\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "# Create a new blob in the bucket\n",
    "blob = bucket.blob('page_views.pkl')\n",
    "# Serialize the object and write it to the blob\n",
    "blob.upload_from_filename(pv_clean)\n",
    "\n",
    "\n",
    "# Create a Storage client\n",
    "storage_client = storage.Client()\n",
    "# Get a reference to the bucket\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "# Get a reference to the blob\n",
    "blob = bucket.blob('page_views.pkl')\n",
    "# Download the contents of the blob\n",
    "data_bytes = blob.download_as_string()\n",
    "# Deserialize the data\n",
    "data = pickle.loads(data_bytes)\n",
    "print(data[57974])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9273910",
   "metadata": {},
   "source": [
    "# ID to title mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e794f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2title = wiki_corpus.select(\"id\", \"title\").toPandas()\n",
    "id2title.index = id2title.id\n",
    "id2title = id2title['title']\n",
    "id2title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file, where you ant to store the data\n",
    "file = open('id2title.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(id2title, file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1679a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [
    "4a5e9a5e",
    "86b303ea",
    "ZgIl6ia_cPD2"
   ],
   "name": "Create_inveted_indexes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
